/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

// clang-format off
#pragma once

#include <tuple>

#include <executorch/runtime/core/exec_aten/exec_aten.h> // at::Tensor etc.
#include <executorch/codegen/macros.h> // TORCH_API
#include <executorch/runtime/kernel/kernel_runtime_context.h>

// @generated by torchgen/gen.py from Functions.h

#include "NativeFunctions.h"

namespace torch {
namespace executor {


namespace aten {

// aten::_cdist_forward.out(Tensor x1, Tensor x2, float p, int? compute_mode, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & _cdist_forward_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & x1, const torch::executor::Tensor & x2, double p, torch::executor::optional<int64_t> compute_mode, torch::executor::Tensor & out) {
    return ::torch::executor::native::_cdist_forward_out(context, x1, x2, p, compute_mode, out);
}


// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & _log_softmax_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, bool half_to_float, torch::executor::Tensor & out) {
    return ::torch::executor::native::log_softmax_out(context, self, dim, half_to_float, out);
}


// aten::_native_batch_norm_legit.out(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps, *, Tensor(d!) out, Tensor(e!) save_mean, Tensor(f!) save_invstd) -> (Tensor(d!), Tensor(e!), Tensor(f!))
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &,torch::executor::Tensor &> _native_batch_norm_legit_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::optional<torch::executor::Tensor> & weight, const torch::executor::optional<torch::executor::Tensor> & bias, torch::executor::Tensor & running_mean, torch::executor::Tensor & running_var, bool training, double momentum, double eps, torch::executor::Tensor & out, torch::executor::Tensor & save_mean, torch::executor::Tensor & save_invstd) {
    return ::torch::executor::native::_native_batch_norm_legit_out(context, input, weight, bias, running_mean, running_var, training, momentum, eps, out, save_mean, save_invstd);
}


// aten::_native_batch_norm_legit.no_stats_out(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &,torch::executor::Tensor &> _native_batch_norm_legit_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::optional<torch::executor::Tensor> & weight, const torch::executor::optional<torch::executor::Tensor> & bias, bool training, double momentum, double eps, torch::executor::Tensor & out, torch::executor::Tensor & save_mean, torch::executor::Tensor & save_invstd) {
    return ::torch::executor::native::_native_batch_norm_legit_no_stats_out(context, input, weight, bias, training, momentum, eps, out, save_mean, save_invstd);
}


// aten::_native_batch_norm_legit_no_training.out(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &,torch::executor::Tensor &> _native_batch_norm_legit_no_training_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::optional<torch::executor::Tensor> & weight, const torch::executor::optional<torch::executor::Tensor> & bias, const torch::executor::Tensor & running_mean, const torch::executor::Tensor & running_var, double momentum, double eps, torch::executor::Tensor & out0, torch::executor::Tensor & out1, torch::executor::Tensor & out2) {
    return ::torch::executor::native::_native_batch_norm_legit_no_training_out(context, input, weight, bias, running_mean, running_var, momentum, eps, out0, out1, out2);
}


// aten::_pdist_forward.out(Tensor self, float p=2, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & _pdist_forward_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, double p, torch::executor::Tensor & out) {
    return ::torch::executor::native::_pdist_forward_out(context, self, p, out);
}


// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & _softmax_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, bool half_to_float, torch::executor::Tensor & out) {
    return ::torch::executor::native::softmax_out(context, self, dim, half_to_float, out);
}


// aten::_to_copy.out(Tensor self, *, bool non_blocking=False, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & _to_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, bool non_blocking, torch::executor::optional<torch::executor::MemoryFormat> memory_format, torch::executor::Tensor & out) {
    return ::torch::executor::native::to_copy_out(context, self, non_blocking, memory_format, out);
}


// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & abs_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::abs_out(context, self, out);
}


// aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & acos_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::acos_out(context, self, out);
}


// aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & acosh_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::acosh_out(context, self, out);
}


// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & add_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, const torch::executor::Scalar & alpha, torch::executor::Tensor & out) {
    return ::torch::executor::native::add_out(context, self, other, alpha, out);
}


// aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & add_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, const torch::executor::Scalar & alpha, torch::executor::Tensor & out) {
    return ::torch::executor::native::add_scalar_out(context, self, other, alpha, out);
}


// aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & addmm_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & mat1, const torch::executor::Tensor & mat2, const torch::executor::Scalar & beta, const torch::executor::Scalar & alpha, torch::executor::Tensor & out) {
    return ::torch::executor::native::addmm_out(context, self, mat1, mat2, beta, alpha, out);
}


// aten::alias_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & alias_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::alias_copy_out(context, self, out);
}


// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & amax_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> dim, bool keepdim, torch::executor::Tensor & out) {
    return ::torch::executor::native::amax_out(context, self, dim, keepdim, out);
}


// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & amin_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> dim, bool keepdim, torch::executor::Tensor & out) {
    return ::torch::executor::native::amin_out(context, self, dim, keepdim, out);
}


// aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & any_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::any_all_out(context, self, out);
}


// aten::any.dims_out(Tensor self, int[]? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & any_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::optional<torch::executor::ArrayRef<int64_t>> dim, bool keepdim, torch::executor::Tensor & out) {
    return ::torch::executor::native::any_dims_out(context, self, dim, keepdim, out);
}


// aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & any_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, bool keepdim, torch::executor::Tensor & out) {
    return ::torch::executor::native::any_out(context, self, dim, keepdim, out);
}


// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & arange_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Scalar & end, torch::executor::Tensor & out) {
    return ::torch::executor::native::arange_out(context, end, out);
}


// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & arange_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Scalar & start, const torch::executor::Scalar & end, const torch::executor::Scalar & step, torch::executor::Tensor & out) {
    return ::torch::executor::native::arange_start_out(context, start, end, step, out);
}


// aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & argmax_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::optional<int64_t> dim, bool keepdim, torch::executor::Tensor & out) {
    return ::torch::executor::native::argmax_out(context, self, dim, keepdim, out);
}


// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & argmin_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::optional<int64_t> dim, bool keepdim, torch::executor::Tensor & out) {
    return ::torch::executor::native::argmin_out(context, self, dim, keepdim, out);
}


// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & as_strided_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> size, torch::executor::ArrayRef<int64_t> stride, torch::executor::optional<int64_t> storage_offset, torch::executor::Tensor & out) {
    return ::torch::executor::native::as_strided_copy_out(context, self, size, stride, storage_offset, out);
}


// aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & asin_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::asin_out(context, self, out);
}


// aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & asinh_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::asinh_out(context, self, out);
}


// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & atan_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::atan_out(context, self, out);
}


// aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & atan2_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::atan2_out(context, self, other, out);
}


// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & atanh_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::atanh_out(context, self, out);
}


// aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & avg_pool2d_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> kernel_size, torch::executor::ArrayRef<int64_t> stride, torch::executor::ArrayRef<int64_t> padding, bool ceil_mode, bool count_include_pad, torch::executor::optional<int64_t> divisor_override, torch::executor::Tensor & out) {
    return ::torch::executor::native::avg_pool2d_out(context, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}


// aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & bitwise_and_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::bitwise_and_Scalar_out(context, self, other, out);
}


// aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & bitwise_and_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::bitwise_and_Tensor_out(context, self, other, out);
}


// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & bitwise_not_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::bitwise_not_out(context, self, out);
}


// aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & bitwise_or_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::bitwise_or_Scalar_out(context, self, other, out);
}


// aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & bitwise_or_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::bitwise_or_Tensor_out(context, self, other, out);
}


// aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & bitwise_xor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::bitwise_xor_Scalar_out(context, self, other, out);
}


// aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & bitwise_xor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::bitwise_xor_Tensor_out(context, self, other, out);
}


// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & bmm_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & mat2, torch::executor::Tensor & out) {
    return ::torch::executor::native::bmm_out(context, self, mat2, out);
}


// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & cat_outf(torch::executor::KernelRuntimeContext & context, torch::executor::TensorList tensors, int64_t dim, torch::executor::Tensor & out) {
    return ::torch::executor::native::cat_out(context, tensors, dim, out);
}


// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & ceil_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::ceil_out(context, self, out);
}


// aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & clamp_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::optional<torch::executor::Scalar> & min, const torch::executor::optional<torch::executor::Scalar> & max, torch::executor::Tensor & out) {
    return ::torch::executor::native::clamp_out(context, self, min, max, out);
}


// aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & clamp_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::optional<torch::executor::Tensor> & min, const torch::executor::optional<torch::executor::Tensor> & max, torch::executor::Tensor & out) {
    return ::torch::executor::native::clamp_tensor_out(context, self, min, max, out);
}


// aten::clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & clone_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::optional<torch::executor::MemoryFormat> memory_format, torch::executor::Tensor & out) {
    return ::torch::executor::native::clone_out(context, self, memory_format, out);
}


// aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & constant_pad_nd_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> pad, const torch::executor::Scalar & value, torch::executor::Tensor & out) {
    return ::torch::executor::native::constant_pad_nd_out(context, self, pad, value, out);
}


// aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & convolution_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & weight, const torch::executor::optional<torch::executor::Tensor> & bias, torch::executor::ArrayRef<int64_t> stride, torch::executor::ArrayRef<int64_t> padding, torch::executor::ArrayRef<int64_t> dilation, bool transposed, torch::executor::ArrayRef<int64_t> output_padding, int64_t groups, torch::executor::Tensor & out) {
    return ::torch::executor::native::convolution_out(context, input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, out);
}


// aten::copy.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & src, bool non_blocking, torch::executor::Tensor & out) {
    return ::torch::executor::native::copy_out(context, self, src, non_blocking, out);
}


// aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & cos_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::cos_out(context, self, out);
}


// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & cosh_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::cosh_out(context, self, out);
}


// aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & cumsum_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::cumsum_out(context, self, dim, dtype, out);
}


// aten::detach_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & detach_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::detach_copy_out(context, self, out);
}


// aten::diagonal_copy.out(Tensor self, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & diagonal_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2, torch::executor::Tensor & out) {
    return ::torch::executor::native::diagonal_copy_out(context, self, offset, dim1, dim2, out);
}


// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & div_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::div_out(context, self, other, out);
}


// aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & div_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::optional<torch::executor::string_view> rounding_mode, torch::executor::Tensor & out) {
    return ::torch::executor::native::div_scalar_mode_out(context, self, other, rounding_mode, out);
}


// aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & div_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::div_scalar_out(context, self, other, out);
}


// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & div_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::optional<torch::executor::string_view> rounding_mode, torch::executor::Tensor & out) {
    return ::torch::executor::native::div_out_mode(context, self, other, rounding_mode, out);
}


// aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & embedding_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse, torch::executor::Tensor & out) {
    return ::torch::executor::native::embedding_out(context, weight, indices, padding_idx, scale_grad_by_freq, sparse, out);
}


// aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & empty_outf(torch::executor::KernelRuntimeContext & context, torch::executor::ArrayRef<int64_t> size, torch::executor::optional<torch::executor::MemoryFormat> memory_format, torch::executor::Tensor & out) {
    return ::torch::executor::native::empty_out(context, size, memory_format, out);
}


// aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & eq_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::eq_scalar_out(context, self, other, out);
}


// aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & eq_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::eq_tensor_out(context, self, other, out);
}


// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & erf_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::erf_out(context, self, out);
}


// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & exp_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::exp_out(context, self, out);
}


// aten::expand_copy.out(Tensor self, SymInt[] size, *, bool implicit=False, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & expand_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> size, bool implicit, torch::executor::Tensor & out) {
    return ::torch::executor::native::expand_copy_out(context, self, size, implicit, out);
}


// aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & expm1_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::expm1_out(context, self, out);
}


// aten::fill.Scalar_out(Tensor self, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & fill_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & value, torch::executor::Tensor & out) {
    return ::torch::executor::native::fill_scalar_out(context, self, value, out);
}


// aten::fill.Tensor_out(Tensor self, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & fill_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & value, torch::executor::Tensor & out) {
    return ::torch::executor::native::fill_tensor_out(context, self, value, out);
}


// aten::flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & flip_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> dims, torch::executor::Tensor & out) {
    return ::torch::executor::native::flip_out(context, self, dims, out);
}


// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & floor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::floor_out(context, self, out);
}


// aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & floor_divide_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::floor_divide_out(context, self, other, out);
}


// aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & fmod_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::fmod_Tensor_out(context, self, other, out);
}


// aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & fmod_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::fmod_Scalar_out(context, self, other, out);
}


// aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & full_outf(torch::executor::KernelRuntimeContext & context, torch::executor::ArrayRef<int64_t> size, const torch::executor::Scalar & fill_value, torch::executor::Tensor & out) {
    return ::torch::executor::native::full_out(context, size, fill_value, out);
}


// aten::full_like.out(Tensor self, Scalar fill_value, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & full_like_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & fill_value, torch::executor::optional<torch::executor::MemoryFormat> memory_format, torch::executor::Tensor & out) {
    return ::torch::executor::native::full_like_out(context, self, fill_value, memory_format, out);
}


// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & ge_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::ge_scalar_out(context, self, other, out);
}


// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & ge_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::ge_tensor_out(context, self, other, out);
}


// aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & gelu_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::string_view approximate, torch::executor::Tensor & out) {
    return ::torch::executor::native::gelu_out(context, self, approximate, out);
}


// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & glu_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, torch::executor::Tensor & out) {
    return ::torch::executor::native::glu_out(context, self, dim, out);
}


// aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & gt_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::gt_scalar_out(context, self, other, out);
}


// aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & gt_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::gt_tensor_out(context, self, other, out);
}


// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & hardtanh_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & min_val, const torch::executor::Scalar & max_val, torch::executor::Tensor & out) {
    return ::torch::executor::native::hardtanh_out(context, self, min_val, max_val, out);
}


// aten::index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & index_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<torch::executor::optional<torch::executor::Tensor>> indices, torch::executor::Tensor & out) {
    return ::torch::executor::native::index_Tensor_out(context, self, indices, out);
}


// aten::index_put.out(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & index_put_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<torch::executor::optional<torch::executor::Tensor>> indices, const torch::executor::Tensor & values, bool accumulate, torch::executor::Tensor & out) {
    return ::torch::executor::native::index_put_out(context, self, indices, values, accumulate, out);
}


// aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & index_select_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, const torch::executor::Tensor & index, torch::executor::Tensor & out) {
    return ::torch::executor::native::index_select_out(context, self, dim, index, out);
}


// aten::isinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & isinf_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::isinf_out(context, self, out);
}


// aten::isnan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & isnan_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::isnan_out(context, self, out);
}


// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & le_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::le_scalar_out(context, self, other, out);
}


// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & le_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::le_tensor_out(context, self, other, out);
}


// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & leaky_relu_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & negative_slope, torch::executor::Tensor & out) {
    return ::torch::executor::native::leaky_relu_out(context, self, negative_slope, out);
}


// aten::lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & lift_fresh_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::lift_fresh_copy_out(context, self, out);
}


// aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & log_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::log_out(context, self, out);
}


// aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & log10_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::log10_out(context, self, out);
}


// aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & log1p_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::log1p_out(context, self, out);
}


// aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & log2_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::log2_out(context, self, out);
}


// aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & logical_and_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::logical_and_out(context, self, other, out);
}


// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & logical_not_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::logical_not_out(context, self, out);
}


// aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & logical_or_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::logical_or_out(context, self, other, out);
}


// aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & logical_xor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::logical_xor_out(context, self, other, out);
}


// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & logit_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::optional<double> eps, torch::executor::Tensor & out) {
    return ::torch::executor::native::logit_out(context, self, eps, out);
}


// aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & lt_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::lt_scalar_out(context, self, other, out);
}


// aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & lt_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::lt_tensor_out(context, self, other, out);
}


// aten::masked_fill.Scalar_out(Tensor self, Tensor mask, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & masked_fill_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & mask, const torch::executor::Scalar & value, torch::executor::Tensor & out) {
    return ::torch::executor::native::masked_fill_scalar_out(context, self, mask, value, out);
}


// aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &> max_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, bool keepdim, torch::executor::Tensor & max, torch::executor::Tensor & max_values) {
    return ::torch::executor::native::max_out(context, self, dim, keepdim, max, max_values);
}


// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & maximum_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::maximum_out(context, self, other, out);
}


// aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &> max_pool2d_with_indices_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> kernel_size, torch::executor::ArrayRef<int64_t> stride, torch::executor::ArrayRef<int64_t> padding, torch::executor::ArrayRef<int64_t> dilation, bool ceil_mode, torch::executor::Tensor & out, torch::executor::Tensor & indices) {
    return ::torch::executor::native::max_pool2d_with_indices_out(context, self, kernel_size, stride, padding, dilation, ceil_mode, out, indices);
}


// aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & mean_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::optional<torch::executor::ArrayRef<int64_t>> dim, bool keepdim, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::mean_dim_out(context, self, dim, keepdim, dtype, out);
}


// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &> min_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, bool keepdim, torch::executor::Tensor & min, torch::executor::Tensor & min_indices) {
    return ::torch::executor::native::min_out(context, self, dim, keepdim, min, min_indices);
}


// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & minimum_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::minimum_out(context, self, other, out);
}


// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & mm_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & mat2, torch::executor::Tensor & out) {
    return ::torch::executor::native::mm_out(context, self, mat2, out);
}


// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & mul_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::mul_out(context, self, other, out);
}


// aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & mul_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::mul_scalar_out(context, self, other, out);
}


// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &,torch::executor::Tensor &> native_group_norm_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::optional<torch::executor::Tensor> & weight, const torch::executor::optional<torch::executor::Tensor> & bias, int64_t N, int64_t C, int64_t HxW, int64_t group, double eps, torch::executor::Tensor & out0, torch::executor::Tensor & out1, torch::executor::Tensor & out2) {
    return ::torch::executor::native::native_group_norm_out(context, input, weight, bias, N, C, HxW, group, eps, out0, out1, out2);
}


// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &,torch::executor::Tensor &> native_layer_norm_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, torch::executor::ArrayRef<int64_t> normalized_shape, const torch::executor::optional<torch::executor::Tensor> & weight, const torch::executor::optional<torch::executor::Tensor> & bias, double eps, torch::executor::Tensor & out0, torch::executor::Tensor & out1, torch::executor::Tensor & out2) {
    return ::torch::executor::native::native_layer_norm_out(context, input, normalized_shape, weight, bias, eps, out0, out1, out2);
}


// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & ne_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::ne_scalar_out(context, self, other, out);
}


// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & ne_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::ne_tensor_out(context, self, other, out);
}


// aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & neg_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::neg_out(context, self, out);
}


// aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & nonzero_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::nonzero_out(context, self, out);
}


// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & ones_outf(torch::executor::KernelRuntimeContext & context, torch::executor::ArrayRef<int64_t> size, torch::executor::Tensor & out) {
    return ::torch::executor::native::ones_out(context, size, out);
}


// aten::permute_copy.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & permute_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> dims, torch::executor::Tensor & out) {
    return ::torch::executor::native::permute_copy_out(context, self, dims, out);
}


// aten::pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & pixel_shuffle_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t upscale_factor, torch::executor::Tensor & out) {
    return ::torch::executor::native::pixel_shuffle_out(context, self, upscale_factor, out);
}


// aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & pow_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Scalar & self, const torch::executor::Tensor & exponent, torch::executor::Tensor & out) {
    return ::torch::executor::native::pow_Scalar_out(context, self, exponent, out);
}


// aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & pow_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & exponent, torch::executor::Tensor & out) {
    return ::torch::executor::native::pow_Tensor_Scalar_out(context, self, exponent, out);
}


// aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & pow_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & exponent, torch::executor::Tensor & out) {
    return ::torch::executor::native::pow_Tensor_Tensor_out(context, self, exponent, out);
}


// aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & prod_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, bool keepdim, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::prod_int_out(context, self, dim, keepdim, dtype, out);
}


// aten::prod.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & prod_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::prod_out(context, self, dtype, out);
}


// aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & reciprocal_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::reciprocal_out(context, self, out);
}


// aten::relu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & relu_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::relu_out(context, self, out);
}


// aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & remainder_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::remainder_Tensor_out(context, self, other, out);
}


// aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & remainder_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::remainder_Scalar_out(context, self, other, out);
}


// aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & repeat_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> repeats, torch::executor::Tensor & out) {
    return ::torch::executor::native::repeat_out(context, self, repeats, out);
}


// aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & reflection_pad1d_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> padding, torch::executor::Tensor & out) {
    return ::torch::executor::native::reflection_pad1d_out(context, self, padding, out);
}


// aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & reflection_pad2d_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> padding, torch::executor::Tensor & out) {
    return ::torch::executor::native::reflection_pad2d_out(context, self, padding, out);
}


// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & reflection_pad3d_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> padding, torch::executor::Tensor & out) {
    return ::torch::executor::native::reflection_pad3d_out(context, self, padding, out);
}


// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & replication_pad1d_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> padding, torch::executor::Tensor & out) {
    return ::torch::executor::native::replication_pad1d_out(context, self, padding, out);
}


// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & replication_pad2d_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> padding, torch::executor::Tensor & out) {
    return ::torch::executor::native::replication_pad2d_out(context, self, padding, out);
}


// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & replication_pad3d_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> padding, torch::executor::Tensor & out) {
    return ::torch::executor::native::replication_pad3d_out(context, self, padding, out);
}


// aten::roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & roll_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> shifts, torch::executor::ArrayRef<int64_t> dims, torch::executor::Tensor & out) {
    return ::torch::executor::native::roll_out(context, self, shifts, dims, out);
}


// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & round_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::round_out(context, self, out);
}


// aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & rsqrt_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::rsqrt_out(context, self, out);
}


// aten::rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & rsub_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, const torch::executor::Scalar & alpha, torch::executor::Tensor & out) {
    return ::torch::executor::native::rsub_scalar_out(context, self, other, alpha, out);
}


// aten::scalar_tensor.out(Scalar s, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & scalar_tensor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Scalar & s, torch::executor::Tensor & out) {
    return ::torch::executor::native::scalar_tensor_out(context, s, out);
}


// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & scatter_add_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, const torch::executor::Tensor & index, const torch::executor::Tensor & src, torch::executor::Tensor & out) {
    return ::torch::executor::native::scatter_add_out(context, self, dim, index, src, out);
}


// aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & select_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, int64_t index, torch::executor::Tensor & out) {
    return ::torch::executor::native::select_copy_int_out(context, self, dim, index, out);
}


// aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & select_scatter_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & src, int64_t dim, int64_t index, torch::executor::Tensor & out) {
    return ::torch::executor::native::select_scatter_out(context, self, src, dim, index, out);
}


// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & sigmoid_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::sigmoid_out(context, self, out);
}


// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & sign_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::sign_out(context, self, out);
}


// aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & sin_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::sin_out(context, self, out);
}


// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & sinh_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::sinh_out(context, self, out);
}


// aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & slice_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, torch::executor::optional<int64_t> start, torch::executor::optional<int64_t> end, int64_t step, torch::executor::Tensor & out) {
    return ::torch::executor::native::slice_copy_Tensor_out(context, self, dim, start, end, step, out);
}


// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & slice_scatter_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & src, int64_t dim, torch::executor::optional<int64_t> start, torch::executor::optional<int64_t> end, int64_t step, torch::executor::Tensor & out) {
    return ::torch::executor::native::slice_scatter_out(context, self, src, dim, start, end, step, out);
}


// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
TORCH_API inline void split_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t split_size, int64_t dim, torch::executor::TensorList out) {
    return ::torch::executor::native::split_copy_Tensor_out(context, self, split_size, dim, out);
}


// aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
TORCH_API inline void split_with_sizes_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> split_sizes, int64_t dim, torch::executor::TensorList out) {
    return ::torch::executor::native::split_with_sizes_copy_out(context, self, split_sizes, dim, out);
}


// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & sqrt_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::sqrt_out(context, self, out);
}


// aten::squeeze_copy.dim_out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & squeeze_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, torch::executor::Tensor & out) {
    return ::torch::executor::native::squeeze_copy_dim_out(context, self, dim, out);
}


// aten::squeeze_copy.dims_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & squeeze_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> dim, torch::executor::Tensor & out) {
    return ::torch::executor::native::squeeze_copy_dims_out(context, self, dim, out);
}


// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & stack_outf(torch::executor::KernelRuntimeContext & context, torch::executor::TensorList tensors, int64_t dim, torch::executor::Tensor & out) {
    return ::torch::executor::native::stack_out(context, tensors, dim, out);
}


// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & sub_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, const torch::executor::Scalar & alpha, torch::executor::Tensor & out) {
    return ::torch::executor::native::sub_out(context, self, other, alpha, out);
}


// aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & sub_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, const torch::executor::Scalar & alpha, torch::executor::Tensor & out) {
    return ::torch::executor::native::sub_scalar_out(context, self, other, alpha, out);
}


// aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & sum_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::optional<torch::executor::ArrayRef<int64_t>> dim, bool keepdim, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::sum_dim_out(context, self, dim, keepdim, dtype, out);
}


// aten::t_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & t_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::t_copy_out(context, self, out);
}


// aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & tan_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::tan_out(context, self, out);
}


// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & tanh_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::tanh_out(context, self, out);
}


// aten::transpose_copy.int_out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & transpose_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim0, int64_t dim1, torch::executor::Tensor & out) {
    return ::torch::executor::native::transpose_copy_int_out(context, self, dim0, dim1, out);
}


// aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & tril_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t diagonal, torch::executor::Tensor & out) {
    return ::torch::executor::native::tril_out(context, self, diagonal, out);
}


// aten::trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & trunc_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::trunc_out(context, self, out);
}


// aten::unbind_copy.int_out(Tensor self, int dim=0, *, Tensor(a!)[] out) -> ()
TORCH_API inline void unbind_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, torch::executor::TensorList out) {
    return ::torch::executor::native::unbind_copy_int_out(context, self, dim, out);
}


// aten::unsqueeze_copy.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & unsqueeze_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, torch::executor::Tensor & out) {
    return ::torch::executor::native::unsqueeze_copy_out(context, self, dim, out);
}


// aten::var.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & var_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::optional<torch::executor::ArrayRef<int64_t>> dim, const torch::executor::optional<torch::executor::Scalar> & correction, bool keepdim, torch::executor::Tensor & out) {
    return ::torch::executor::native::var_correction_out(context, self, dim, correction, keepdim, out);
}


// aten::var.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & var_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::optional<torch::executor::ArrayRef<int64_t>> dim, bool unbiased, bool keepdim, torch::executor::Tensor & out) {
    return ::torch::executor::native::var_out(context, self, dim, unbiased, keepdim, out);
}


// aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & view_copy_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> size, torch::executor::Tensor & out) {
    return ::torch::executor::native::view_copy_out(context, self, size, out);
}


// aten::where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & where_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & condition, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::where_out(context, condition, self, other, out);
}


// aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & zeros_outf(torch::executor::KernelRuntimeContext & context, torch::executor::ArrayRef<int64_t> size, torch::executor::Tensor & out) {
    return ::torch::executor::native::zeros_out(context, size, out);
}

} // namespace aten

} // namespace executor
} // namespace torch
