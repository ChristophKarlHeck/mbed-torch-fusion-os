/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

#include <executorch/runtime/core/evalue.h>
#include <executorch/runtime/core/exec_aten/exec_aten.h>
#include <executorch/runtime/kernel/operator_registry.h>
#include <executorch/runtime/platform/profiler.h>
#include "NativeFunctions.h" // Generated Function import headers
// @generated by torchgen/gen.py from RegisterCodegenUnboxedKernels.cpp

// NOTE [Sharded File]: This file is generated in a sharded fashion to speed up
// incremental rebuilds. See the comment at the top of
// templates/VariableType.cpp for an analogous, in-depth discussion.
//
// Generated by tools/jit/gen_unboxing.py. This file registers all ATen ops into
// JIT op registry instead of c10 dispatcher. JIT op registry only takes boxed
// kernels, so we are calling unboxing functions in UnboxingFunctions.h to cast
// arguments into C++ types (instead of IValue) and delegate to unboxed kernels.
using KernelArrayRef = ::torch::executor::ArrayRef<::torch::executor::Kernel>;
namespace torch {
namespace executor {
namespace function {
namespace {

static Kernel kernels_to_register[] = {
    
    Kernel(
        "aten::_cdist_forward.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& x1 = *stack[0];
    	EValue& x2 = *stack[1];
    	EValue& p = *stack[2];
    	EValue& compute_mode = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & x1_base = x1.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & x2_base = x2.to<torch::executor::Tensor>();
    	double p_base = p.to<double>();
    	
    	    torch::executor::optional<int64_t> compute_mode_opt_out = compute_mode.toOptional<int64_t>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call__cdist_forward.out");
            EXECUTORCH_SCOPE_PROF("native_call__cdist_forward.out");
            torch::executor::native::_cdist_forward_out(context, x1_base, x2_base, p_base, compute_mode_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::_log_softmax.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& half_to_float = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	bool half_to_float_base = half_to_float.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call__log_softmax.out");
            EXECUTORCH_SCOPE_PROF("native_call__log_softmax.out");
            torch::executor::native::log_softmax_out(context, self_base, dim_base, half_to_float_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::_native_batch_norm_legit.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& weight = *stack[1];
    	EValue& bias = *stack[2];
    	EValue& running_mean = *stack[3];
    	EValue& running_var = *stack[4];
    	EValue& training = *stack[5];
    	EValue& momentum = *stack[6];
    	EValue& eps = *stack[7];
    	EValue& out = *stack[8];
    	EValue& save_mean = *stack[9];
    	EValue& save_invstd = *stack[10];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::Tensor> weight_opt_out = weight.toOptional<torch::executor::Tensor>();
    	            
    	
    	    torch::executor::optional<torch::executor::Tensor> bias_opt_out = bias.toOptional<torch::executor::Tensor>();
    	            
    	torch::executor::Tensor & running_mean_base = running_mean.to<torch::executor::Tensor>();
    	torch::executor::Tensor & running_var_base = running_var.to<torch::executor::Tensor>();
    	bool training_base = training.to<bool>();
    	double momentum_base = momentum.to<double>();
    	double eps_base = eps.to<double>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    	torch::executor::Tensor & save_mean_base = save_mean.to<torch::executor::Tensor>();
    	torch::executor::Tensor & save_invstd_base = save_invstd.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call__native_batch_norm_legit.out");
            EXECUTORCH_SCOPE_PROF("native_call__native_batch_norm_legit.out");
            torch::executor::native::_native_batch_norm_legit_out(context, input_base, weight_opt_out, bias_opt_out, running_mean_base, running_var_base, training_base, momentum_base, eps_base, out_base, save_mean_base, save_invstd_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[8]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[9]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[10]);
    
            
        }
    ),
    
    Kernel(
        "aten::_native_batch_norm_legit.no_stats_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& weight = *stack[1];
    	EValue& bias = *stack[2];
    	EValue& training = *stack[3];
    	EValue& momentum = *stack[4];
    	EValue& eps = *stack[5];
    	EValue& out = *stack[6];
    	EValue& save_mean = *stack[7];
    	EValue& save_invstd = *stack[8];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::Tensor> weight_opt_out = weight.toOptional<torch::executor::Tensor>();
    	            
    	
    	    torch::executor::optional<torch::executor::Tensor> bias_opt_out = bias.toOptional<torch::executor::Tensor>();
    	            
    	bool training_base = training.to<bool>();
    	double momentum_base = momentum.to<double>();
    	double eps_base = eps.to<double>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    	torch::executor::Tensor & save_mean_base = save_mean.to<torch::executor::Tensor>();
    	torch::executor::Tensor & save_invstd_base = save_invstd.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call__native_batch_norm_legit.no_stats_out");
            EXECUTORCH_SCOPE_PROF("native_call__native_batch_norm_legit.no_stats_out");
            torch::executor::native::_native_batch_norm_legit_no_stats_out(context, input_base, weight_opt_out, bias_opt_out, training_base, momentum_base, eps_base, out_base, save_mean_base, save_invstd_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[8]);
    
            
        }
    ),
    
    Kernel(
        "aten::_native_batch_norm_legit_no_training.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& weight = *stack[1];
    	EValue& bias = *stack[2];
    	EValue& running_mean = *stack[3];
    	EValue& running_var = *stack[4];
    	EValue& momentum = *stack[5];
    	EValue& eps = *stack[6];
    	EValue& out0 = *stack[7];
    	EValue& out1 = *stack[8];
    	EValue& out2 = *stack[9];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::Tensor> weight_opt_out = weight.toOptional<torch::executor::Tensor>();
    	            
    	
    	    torch::executor::optional<torch::executor::Tensor> bias_opt_out = bias.toOptional<torch::executor::Tensor>();
    	            
    	const torch::executor::Tensor & running_mean_base = running_mean.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & running_var_base = running_var.to<torch::executor::Tensor>();
    	double momentum_base = momentum.to<double>();
    	double eps_base = eps.to<double>();
    	torch::executor::Tensor & out0_base = out0.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out1_base = out1.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out2_base = out2.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call__native_batch_norm_legit_no_training.out");
            EXECUTORCH_SCOPE_PROF("native_call__native_batch_norm_legit_no_training.out");
            torch::executor::native::_native_batch_norm_legit_no_training_out(context, input_base, weight_opt_out, bias_opt_out, running_mean_base, running_var_base, momentum_base, eps_base, out0_base, out1_base, out2_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[8]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[9]);
    
            
        }
    ),
    
    Kernel(
        "aten::_pdist_forward.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& p = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	double p_base = p.to<double>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call__pdist_forward.out");
            EXECUTORCH_SCOPE_PROF("native_call__pdist_forward.out");
            torch::executor::native::_pdist_forward_out(context, self_base, p_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::_softmax.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& half_to_float = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	bool half_to_float_base = half_to_float.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call__softmax.out");
            EXECUTORCH_SCOPE_PROF("native_call__softmax.out");
            torch::executor::native::softmax_out(context, self_base, dim_base, half_to_float_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::_to_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& non_blocking = *stack[1];
    	EValue& memory_format = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	bool non_blocking_base = non_blocking.to<bool>();
    	
    	    torch::executor::optional<torch::executor::MemoryFormat> memory_format_opt_out = memory_format.toOptional<torch::executor::MemoryFormat>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call__to_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call__to_copy.out");
            torch::executor::native::to_copy_out(context, self_base, non_blocking_base, memory_format_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::abs.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_abs.out");
            EXECUTORCH_SCOPE_PROF("native_call_abs.out");
            torch::executor::native::abs_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::acos.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_acos.out");
            EXECUTORCH_SCOPE_PROF("native_call_acos.out");
            torch::executor::native::acos_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::acosh.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_acosh.out");
            EXECUTORCH_SCOPE_PROF("native_call_acosh.out");
            torch::executor::native::acosh_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::add.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& alpha = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & alpha_base = alpha.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_add.out");
            EXECUTORCH_SCOPE_PROF("native_call_add.out");
            torch::executor::native::add_out(context, self_base, other_base, alpha_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::add.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& alpha = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & alpha_base = alpha.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_add.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_add.Scalar_out");
            torch::executor::native::add_scalar_out(context, self_base, other_base, alpha_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::addmm.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& mat1 = *stack[1];
    	EValue& mat2 = *stack[2];
    	EValue& beta = *stack[3];
    	EValue& alpha = *stack[4];
    	EValue& out = *stack[5];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & mat1_base = mat1.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & mat2_base = mat2.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & beta_base = beta.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & alpha_base = alpha.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_addmm.out");
            EXECUTORCH_SCOPE_PROF("native_call_addmm.out");
            torch::executor::native::addmm_out(context, self_base, mat1_base, mat2_base, beta_base, alpha_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[5]);
    
            
        }
    ),
    
    Kernel(
        "aten::alias_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_alias_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_alias_copy.out");
            torch::executor::native::alias_copy_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::amax.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> dim_list_out = dim.toIntList();
    	                
    	bool keepdim_base = keepdim.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_amax.out");
            EXECUTORCH_SCOPE_PROF("native_call_amax.out");
            torch::executor::native::amax_out(context, self_base, dim_list_out, keepdim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::amin.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> dim_list_out = dim.toIntList();
    	                
    	bool keepdim_base = keepdim.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_amin.out");
            EXECUTORCH_SCOPE_PROF("native_call_amin.out");
            torch::executor::native::amin_out(context, self_base, dim_list_out, keepdim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::any.all_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_any.all_out");
            EXECUTORCH_SCOPE_PROF("native_call_any.all_out");
            torch::executor::native::any_all_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::any.dims_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::ArrayRef<int64_t>> dim_opt_out = dim.toOptional<torch::executor::ArrayRef<int64_t>>();
    	            
    	bool keepdim_base = keepdim.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_any.dims_out");
            EXECUTORCH_SCOPE_PROF("native_call_any.dims_out");
            torch::executor::native::any_dims_out(context, self_base, dim_opt_out, keepdim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::any.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	bool keepdim_base = keepdim.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_any.out");
            EXECUTORCH_SCOPE_PROF("native_call_any.out");
            torch::executor::native::any_out(context, self_base, dim_base, keepdim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::arange.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& end = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Scalar & end_base = end.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_arange.out");
            EXECUTORCH_SCOPE_PROF("native_call_arange.out");
            torch::executor::native::arange_out(context, end_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::arange.start_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& start = *stack[0];
    	EValue& end = *stack[1];
    	EValue& step = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Scalar & start_base = start.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & end_base = end.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & step_base = step.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_arange.start_out");
            EXECUTORCH_SCOPE_PROF("native_call_arange.start_out");
            torch::executor::native::arange_start_out(context, start_base, end_base, step_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::argmax.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<int64_t> dim_opt_out = dim.toOptional<int64_t>();
    	            
    	bool keepdim_base = keepdim.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_argmax.out");
            EXECUTORCH_SCOPE_PROF("native_call_argmax.out");
            torch::executor::native::argmax_out(context, self_base, dim_opt_out, keepdim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::argmin.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<int64_t> dim_opt_out = dim.toOptional<int64_t>();
    	            
    	bool keepdim_base = keepdim.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_argmin.out");
            EXECUTORCH_SCOPE_PROF("native_call_argmin.out");
            torch::executor::native::argmin_out(context, self_base, dim_opt_out, keepdim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::as_strided_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& size = *stack[1];
    	EValue& stride = *stack[2];
    	EValue& storage_offset = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> size_list_out = size.toIntList();
    	                
    	
    	    torch::executor::ArrayRef<int64_t> stride_list_out = stride.toIntList();
    	                
    	
    	    torch::executor::optional<int64_t> storage_offset_opt_out = storage_offset.toOptional<int64_t>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_as_strided_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_as_strided_copy.out");
            torch::executor::native::as_strided_copy_out(context, self_base, size_list_out, stride_list_out, storage_offset_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::asin.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_asin.out");
            EXECUTORCH_SCOPE_PROF("native_call_asin.out");
            torch::executor::native::asin_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::asinh.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_asinh.out");
            EXECUTORCH_SCOPE_PROF("native_call_asinh.out");
            torch::executor::native::asinh_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::atan.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_atan.out");
            EXECUTORCH_SCOPE_PROF("native_call_atan.out");
            torch::executor::native::atan_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::atan2.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_atan2.out");
            EXECUTORCH_SCOPE_PROF("native_call_atan2.out");
            torch::executor::native::atan2_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::atanh.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_atanh.out");
            EXECUTORCH_SCOPE_PROF("native_call_atanh.out");
            torch::executor::native::atanh_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::avg_pool2d.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& kernel_size = *stack[1];
    	EValue& stride = *stack[2];
    	EValue& padding = *stack[3];
    	EValue& ceil_mode = *stack[4];
    	EValue& count_include_pad = *stack[5];
    	EValue& divisor_override = *stack[6];
    	EValue& out = *stack[7];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> kernel_size_list_out = kernel_size.toIntList();
    	                
    	
    	    torch::executor::ArrayRef<int64_t> stride_list_out = stride.toIntList();
    	                
    	
    	    torch::executor::ArrayRef<int64_t> padding_list_out = padding.toIntList();
    	                
    	bool ceil_mode_base = ceil_mode.to<bool>();
    	bool count_include_pad_base = count_include_pad.to<bool>();
    	
    	    torch::executor::optional<int64_t> divisor_override_opt_out = divisor_override.toOptional<int64_t>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_avg_pool2d.out");
            EXECUTORCH_SCOPE_PROF("native_call_avg_pool2d.out");
            torch::executor::native::avg_pool2d_out(context, self_base, kernel_size_list_out, stride_list_out, padding_list_out, ceil_mode_base, count_include_pad_base, divisor_override_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    
            
        }
    ),
    
    Kernel(
        "aten::bitwise_and.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_bitwise_and.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_bitwise_and.Scalar_out");
            torch::executor::native::bitwise_and_Scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::bitwise_and.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_bitwise_and.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_bitwise_and.Tensor_out");
            torch::executor::native::bitwise_and_Tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::bitwise_not.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_bitwise_not.out");
            EXECUTORCH_SCOPE_PROF("native_call_bitwise_not.out");
            torch::executor::native::bitwise_not_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::bitwise_or.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_bitwise_or.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_bitwise_or.Scalar_out");
            torch::executor::native::bitwise_or_Scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::bitwise_or.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_bitwise_or.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_bitwise_or.Tensor_out");
            torch::executor::native::bitwise_or_Tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::bitwise_xor.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_bitwise_xor.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_bitwise_xor.Scalar_out");
            torch::executor::native::bitwise_xor_Scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::bitwise_xor.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_bitwise_xor.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_bitwise_xor.Tensor_out");
            torch::executor::native::bitwise_xor_Tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::bmm.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& mat2 = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & mat2_base = mat2.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_bmm.out");
            EXECUTORCH_SCOPE_PROF("native_call_bmm.out");
            torch::executor::native::bmm_out(context, self_base, mat2_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::cat.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& tensors = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& out = *stack[2];
    	
    	    torch::executor::TensorList tensors_list_out = tensors.toTensorList();
    	                
    	int64_t dim_base = dim.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_cat.out");
            EXECUTORCH_SCOPE_PROF("native_call_cat.out");
            torch::executor::native::cat_out(context, tensors_list_out, dim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::ceil.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_ceil.out");
            EXECUTORCH_SCOPE_PROF("native_call_ceil.out");
            torch::executor::native::ceil_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::clamp.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& min = *stack[1];
    	EValue& max = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::Scalar> min_opt_out = min.toOptional<torch::executor::Scalar>();
    	            
    	
    	    torch::executor::optional<torch::executor::Scalar> max_opt_out = max.toOptional<torch::executor::Scalar>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_clamp.out");
            EXECUTORCH_SCOPE_PROF("native_call_clamp.out");
            torch::executor::native::clamp_out(context, self_base, min_opt_out, max_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::clamp.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& min = *stack[1];
    	EValue& max = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::Tensor> min_opt_out = min.toOptional<torch::executor::Tensor>();
    	            
    	
    	    torch::executor::optional<torch::executor::Tensor> max_opt_out = max.toOptional<torch::executor::Tensor>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_clamp.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_clamp.Tensor_out");
            torch::executor::native::clamp_tensor_out(context, self_base, min_opt_out, max_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::clone.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& memory_format = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::MemoryFormat> memory_format_opt_out = memory_format.toOptional<torch::executor::MemoryFormat>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_clone.out");
            EXECUTORCH_SCOPE_PROF("native_call_clone.out");
            torch::executor::native::clone_out(context, self_base, memory_format_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::constant_pad_nd.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& pad = *stack[1];
    	EValue& value = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> pad_list_out = pad.toIntList();
    	                
    	const torch::executor::Scalar & value_base = value.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_constant_pad_nd.out");
            EXECUTORCH_SCOPE_PROF("native_call_constant_pad_nd.out");
            torch::executor::native::constant_pad_nd_out(context, self_base, pad_list_out, value_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::convolution.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& weight = *stack[1];
    	EValue& bias = *stack[2];
    	EValue& stride = *stack[3];
    	EValue& padding = *stack[4];
    	EValue& dilation = *stack[5];
    	EValue& transposed = *stack[6];
    	EValue& output_padding = *stack[7];
    	EValue& groups = *stack[8];
    	EValue& out = *stack[9];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_base = weight.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::Tensor> bias_opt_out = bias.toOptional<torch::executor::Tensor>();
    	            
    	
    	    torch::executor::ArrayRef<int64_t> stride_list_out = stride.toIntList();
    	                
    	
    	    torch::executor::ArrayRef<int64_t> padding_list_out = padding.toIntList();
    	                
    	
    	    torch::executor::ArrayRef<int64_t> dilation_list_out = dilation.toIntList();
    	                
    	bool transposed_base = transposed.to<bool>();
    	
    	    torch::executor::ArrayRef<int64_t> output_padding_list_out = output_padding.toIntList();
    	                
    	int64_t groups_base = groups.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_convolution.out");
            EXECUTORCH_SCOPE_PROF("native_call_convolution.out");
            torch::executor::native::convolution_out(context, input_base, weight_base, bias_opt_out, stride_list_out, padding_list_out, dilation_list_out, transposed_base, output_padding_list_out, groups_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[9]);
    
            
        }
    ),
    
    Kernel(
        "aten::copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& src = *stack[1];
    	EValue& non_blocking = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & src_base = src.to<torch::executor::Tensor>();
    	bool non_blocking_base = non_blocking.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_copy.out");
            torch::executor::native::copy_out(context, self_base, src_base, non_blocking_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::cos.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_cos.out");
            EXECUTORCH_SCOPE_PROF("native_call_cos.out");
            torch::executor::native::cos_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::cosh.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_cosh.out");
            EXECUTORCH_SCOPE_PROF("native_call_cosh.out");
            torch::executor::native::cosh_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::cumsum.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& dtype = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	
    	    torch::executor::optional<torch::executor::ScalarType> dtype_opt_out = dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_cumsum.out");
            EXECUTORCH_SCOPE_PROF("native_call_cumsum.out");
            torch::executor::native::cumsum_out(context, self_base, dim_base, dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::detach_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_detach_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_detach_copy.out");
            torch::executor::native::detach_copy_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::diagonal_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& offset = *stack[1];
    	EValue& dim1 = *stack[2];
    	EValue& dim2 = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t offset_base = offset.to<int64_t>();
    	int64_t dim1_base = dim1.to<int64_t>();
    	int64_t dim2_base = dim2.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_diagonal_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_diagonal_copy.out");
            torch::executor::native::diagonal_copy_out(context, self_base, offset_base, dim1_base, dim2_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::div.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_div.out");
            EXECUTORCH_SCOPE_PROF("native_call_div.out");
            torch::executor::native::div_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::div.Scalar_mode_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& rounding_mode = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	
    	    torch::executor::optional<torch::executor::string_view> rounding_mode_opt_out = rounding_mode.toOptional<torch::executor::string_view>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_div.Scalar_mode_out");
            EXECUTORCH_SCOPE_PROF("native_call_div.Scalar_mode_out");
            torch::executor::native::div_scalar_mode_out(context, self_base, other_base, rounding_mode_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::div.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_div.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_div.Scalar_out");
            torch::executor::native::div_scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::div.out_mode",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& rounding_mode = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::string_view> rounding_mode_opt_out = rounding_mode.toOptional<torch::executor::string_view>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_div.out_mode");
            EXECUTORCH_SCOPE_PROF("native_call_div.out_mode");
            torch::executor::native::div_out_mode(context, self_base, other_base, rounding_mode_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::embedding.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& weight = *stack[0];
    	EValue& indices = *stack[1];
    	EValue& padding_idx = *stack[2];
    	EValue& scale_grad_by_freq = *stack[3];
    	EValue& sparse = *stack[4];
    	EValue& out = *stack[5];
    	const torch::executor::Tensor & weight_base = weight.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & indices_base = indices.to<torch::executor::Tensor>();
    	int64_t padding_idx_base = padding_idx.to<int64_t>();
    	bool scale_grad_by_freq_base = scale_grad_by_freq.to<bool>();
    	bool sparse_base = sparse.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_embedding.out");
            EXECUTORCH_SCOPE_PROF("native_call_embedding.out");
            torch::executor::native::embedding_out(context, weight_base, indices_base, padding_idx_base, scale_grad_by_freq_base, sparse_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[5]);
    
            
        }
    ),
    
    Kernel(
        "aten::empty.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& size = *stack[0];
    	EValue& memory_format = *stack[1];
    	EValue& out = *stack[2];
    	
    	    torch::executor::ArrayRef<int64_t> size_list_out = size.toIntList();
    	                
    	
    	    torch::executor::optional<torch::executor::MemoryFormat> memory_format_opt_out = memory_format.toOptional<torch::executor::MemoryFormat>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_empty.out");
            EXECUTORCH_SCOPE_PROF("native_call_empty.out");
            torch::executor::native::empty_out(context, size_list_out, memory_format_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::eq.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_eq.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_eq.Scalar_out");
            torch::executor::native::eq_scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::eq.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_eq.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_eq.Tensor_out");
            torch::executor::native::eq_tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::erf.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_erf.out");
            EXECUTORCH_SCOPE_PROF("native_call_erf.out");
            torch::executor::native::erf_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::exp.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_exp.out");
            EXECUTORCH_SCOPE_PROF("native_call_exp.out");
            torch::executor::native::exp_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::expand_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& size = *stack[1];
    	EValue& implicit = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> size_list_out = size.toIntList();
    	                
    	bool implicit_base = implicit.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_expand_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_expand_copy.out");
            torch::executor::native::expand_copy_out(context, self_base, size_list_out, implicit_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::expm1.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_expm1.out");
            EXECUTORCH_SCOPE_PROF("native_call_expm1.out");
            torch::executor::native::expm1_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::fill.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& value = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & value_base = value.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_fill.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_fill.Scalar_out");
            torch::executor::native::fill_scalar_out(context, self_base, value_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::fill.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& value = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & value_base = value.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_fill.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_fill.Tensor_out");
            torch::executor::native::fill_tensor_out(context, self_base, value_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::flip.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dims = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> dims_list_out = dims.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_flip.out");
            EXECUTORCH_SCOPE_PROF("native_call_flip.out");
            torch::executor::native::flip_out(context, self_base, dims_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::floor.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_floor.out");
            EXECUTORCH_SCOPE_PROF("native_call_floor.out");
            torch::executor::native::floor_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::floor_divide.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_floor_divide.out");
            EXECUTORCH_SCOPE_PROF("native_call_floor_divide.out");
            torch::executor::native::floor_divide_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::fmod.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_fmod.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_fmod.Tensor_out");
            torch::executor::native::fmod_Tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::fmod.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_fmod.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_fmod.Scalar_out");
            torch::executor::native::fmod_Scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::full.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& size = *stack[0];
    	EValue& fill_value = *stack[1];
    	EValue& out = *stack[2];
    	
    	    torch::executor::ArrayRef<int64_t> size_list_out = size.toIntList();
    	                
    	const torch::executor::Scalar & fill_value_base = fill_value.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_full.out");
            EXECUTORCH_SCOPE_PROF("native_call_full.out");
            torch::executor::native::full_out(context, size_list_out, fill_value_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::full_like.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& fill_value = *stack[1];
    	EValue& memory_format = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & fill_value_base = fill_value.to<torch::executor::Scalar>();
    	
    	    torch::executor::optional<torch::executor::MemoryFormat> memory_format_opt_out = memory_format.toOptional<torch::executor::MemoryFormat>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_full_like.out");
            EXECUTORCH_SCOPE_PROF("native_call_full_like.out");
            torch::executor::native::full_like_out(context, self_base, fill_value_base, memory_format_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::ge.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_ge.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_ge.Scalar_out");
            torch::executor::native::ge_scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::ge.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_ge.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_ge.Tensor_out");
            torch::executor::native::ge_tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::gelu.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& approximate = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::string_view approximate_base = approximate.to<torch::executor::string_view>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_gelu.out");
            EXECUTORCH_SCOPE_PROF("native_call_gelu.out");
            torch::executor::native::gelu_out(context, self_base, approximate_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::glu.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_glu.out");
            EXECUTORCH_SCOPE_PROF("native_call_glu.out");
            torch::executor::native::glu_out(context, self_base, dim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::gt.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_gt.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_gt.Scalar_out");
            torch::executor::native::gt_scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::gt.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_gt.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_gt.Tensor_out");
            torch::executor::native::gt_tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::hardtanh.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& min_val = *stack[1];
    	EValue& max_val = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & min_val_base = min_val.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & max_val_base = max_val.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_hardtanh.out");
            EXECUTORCH_SCOPE_PROF("native_call_hardtanh.out");
            torch::executor::native::hardtanh_out(context, self_base, min_val_base, max_val_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::index.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& indices = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	#ifdef USE_ATEN_LIB
    	at::ArrayRef<c10::optional<at::Tensor>> indices_list_in = indices.toListOptionalTensor();
    	c10::List<c10::optional<at::Tensor>> indices_list_out;
    	for (auto indices_elem: indices_list_in) {
    	    indices_list_out.push_back(indices_elem);
    	}
    	#else
    	torch::executor::ArrayRef<torch::executor::optional<torch::executor::Tensor>> indices_list_out = indices.toListOptionalTensor();
    	#endif
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_index.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_index.Tensor_out");
            torch::executor::native::index_Tensor_out(context, self_base, indices_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::index_put.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& indices = *stack[1];
    	EValue& values = *stack[2];
    	EValue& accumulate = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	#ifdef USE_ATEN_LIB
    	at::ArrayRef<c10::optional<at::Tensor>> indices_list_in = indices.toListOptionalTensor();
    	c10::List<c10::optional<at::Tensor>> indices_list_out;
    	for (auto indices_elem: indices_list_in) {
    	    indices_list_out.push_back(indices_elem);
    	}
    	#else
    	torch::executor::ArrayRef<torch::executor::optional<torch::executor::Tensor>> indices_list_out = indices.toListOptionalTensor();
    	#endif
    	                
    	const torch::executor::Tensor & values_base = values.to<torch::executor::Tensor>();
    	bool accumulate_base = accumulate.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_index_put.out");
            EXECUTORCH_SCOPE_PROF("native_call_index_put.out");
            torch::executor::native::index_put_out(context, self_base, indices_list_out, values_base, accumulate_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::index_select.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& index = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	const torch::executor::Tensor & index_base = index.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_index_select.out");
            EXECUTORCH_SCOPE_PROF("native_call_index_select.out");
            torch::executor::native::index_select_out(context, self_base, dim_base, index_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::isinf.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_isinf.out");
            EXECUTORCH_SCOPE_PROF("native_call_isinf.out");
            torch::executor::native::isinf_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::isnan.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_isnan.out");
            EXECUTORCH_SCOPE_PROF("native_call_isnan.out");
            torch::executor::native::isnan_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::le.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_le.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_le.Scalar_out");
            torch::executor::native::le_scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::le.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_le.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_le.Tensor_out");
            torch::executor::native::le_tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::leaky_relu.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& negative_slope = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & negative_slope_base = negative_slope.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_leaky_relu.out");
            EXECUTORCH_SCOPE_PROF("native_call_leaky_relu.out");
            torch::executor::native::leaky_relu_out(context, self_base, negative_slope_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::lift_fresh_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_lift_fresh_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_lift_fresh_copy.out");
            torch::executor::native::lift_fresh_copy_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::log.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_log.out");
            EXECUTORCH_SCOPE_PROF("native_call_log.out");
            torch::executor::native::log_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::log10.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_log10.out");
            EXECUTORCH_SCOPE_PROF("native_call_log10.out");
            torch::executor::native::log10_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::log1p.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_log1p.out");
            EXECUTORCH_SCOPE_PROF("native_call_log1p.out");
            torch::executor::native::log1p_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::log2.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_log2.out");
            EXECUTORCH_SCOPE_PROF("native_call_log2.out");
            torch::executor::native::log2_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::logical_and.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_logical_and.out");
            EXECUTORCH_SCOPE_PROF("native_call_logical_and.out");
            torch::executor::native::logical_and_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::logical_not.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_logical_not.out");
            EXECUTORCH_SCOPE_PROF("native_call_logical_not.out");
            torch::executor::native::logical_not_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::logical_or.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_logical_or.out");
            EXECUTORCH_SCOPE_PROF("native_call_logical_or.out");
            torch::executor::native::logical_or_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::logical_xor.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_logical_xor.out");
            EXECUTORCH_SCOPE_PROF("native_call_logical_xor.out");
            torch::executor::native::logical_xor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::logit.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& eps = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<double> eps_opt_out = eps.toOptional<double>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_logit.out");
            EXECUTORCH_SCOPE_PROF("native_call_logit.out");
            torch::executor::native::logit_out(context, self_base, eps_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::lt.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_lt.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_lt.Scalar_out");
            torch::executor::native::lt_scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::lt.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_lt.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_lt.Tensor_out");
            torch::executor::native::lt_tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::masked_fill.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& mask = *stack[1];
    	EValue& value = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & mask_base = mask.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & value_base = value.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_masked_fill.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_masked_fill.Scalar_out");
            torch::executor::native::masked_fill_scalar_out(context, self_base, mask_base, value_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::max.dim_max",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& max = *stack[3];
    	EValue& max_values = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	bool keepdim_base = keepdim.to<bool>();
    	torch::executor::Tensor & max_base = max.to<torch::executor::Tensor>();
    	torch::executor::Tensor & max_values_base = max_values.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_max.dim_max");
            EXECUTORCH_SCOPE_PROF("native_call_max.dim_max");
            torch::executor::native::max_out(context, self_base, dim_base, keepdim_base, max_base, max_values_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::maximum.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_maximum.out");
            EXECUTORCH_SCOPE_PROF("native_call_maximum.out");
            torch::executor::native::maximum_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::max_pool2d_with_indices.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& kernel_size = *stack[1];
    	EValue& stride = *stack[2];
    	EValue& padding = *stack[3];
    	EValue& dilation = *stack[4];
    	EValue& ceil_mode = *stack[5];
    	EValue& out = *stack[6];
    	EValue& indices = *stack[7];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> kernel_size_list_out = kernel_size.toIntList();
    	                
    	
    	    torch::executor::ArrayRef<int64_t> stride_list_out = stride.toIntList();
    	                
    	
    	    torch::executor::ArrayRef<int64_t> padding_list_out = padding.toIntList();
    	                
    	
    	    torch::executor::ArrayRef<int64_t> dilation_list_out = dilation.toIntList();
    	                
    	bool ceil_mode_base = ceil_mode.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    	torch::executor::Tensor & indices_base = indices.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_max_pool2d_with_indices.out");
            EXECUTORCH_SCOPE_PROF("native_call_max_pool2d_with_indices.out");
            torch::executor::native::max_pool2d_with_indices_out(context, self_base, kernel_size_list_out, stride_list_out, padding_list_out, dilation_list_out, ceil_mode_base, out_base, indices_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    
            
        }
    ),
    
    Kernel(
        "aten::mean.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& dtype = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::ArrayRef<int64_t>> dim_opt_out = dim.toOptional<torch::executor::ArrayRef<int64_t>>();
    	            
    	bool keepdim_base = keepdim.to<bool>();
    	
    	    torch::executor::optional<torch::executor::ScalarType> dtype_opt_out = dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_mean.out");
            EXECUTORCH_SCOPE_PROF("native_call_mean.out");
            torch::executor::native::mean_dim_out(context, self_base, dim_opt_out, keepdim_base, dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::min.dim_min",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& min = *stack[3];
    	EValue& min_indices = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	bool keepdim_base = keepdim.to<bool>();
    	torch::executor::Tensor & min_base = min.to<torch::executor::Tensor>();
    	torch::executor::Tensor & min_indices_base = min_indices.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_min.dim_min");
            EXECUTORCH_SCOPE_PROF("native_call_min.dim_min");
            torch::executor::native::min_out(context, self_base, dim_base, keepdim_base, min_base, min_indices_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::minimum.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_minimum.out");
            EXECUTORCH_SCOPE_PROF("native_call_minimum.out");
            torch::executor::native::minimum_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::mm.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& mat2 = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & mat2_base = mat2.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_mm.out");
            EXECUTORCH_SCOPE_PROF("native_call_mm.out");
            torch::executor::native::mm_out(context, self_base, mat2_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::mul.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_mul.out");
            EXECUTORCH_SCOPE_PROF("native_call_mul.out");
            torch::executor::native::mul_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::mul.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_mul.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_mul.Scalar_out");
            torch::executor::native::mul_scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::native_group_norm.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& weight = *stack[1];
    	EValue& bias = *stack[2];
    	EValue& N = *stack[3];
    	EValue& C = *stack[4];
    	EValue& HxW = *stack[5];
    	EValue& group = *stack[6];
    	EValue& eps = *stack[7];
    	EValue& out0 = *stack[8];
    	EValue& out1 = *stack[9];
    	EValue& out2 = *stack[10];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::Tensor> weight_opt_out = weight.toOptional<torch::executor::Tensor>();
    	            
    	
    	    torch::executor::optional<torch::executor::Tensor> bias_opt_out = bias.toOptional<torch::executor::Tensor>();
    	            
    	int64_t N_base = N.to<int64_t>();
    	int64_t C_base = C.to<int64_t>();
    	int64_t HxW_base = HxW.to<int64_t>();
    	int64_t group_base = group.to<int64_t>();
    	double eps_base = eps.to<double>();
    	torch::executor::Tensor & out0_base = out0.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out1_base = out1.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out2_base = out2.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_native_group_norm.out");
            EXECUTORCH_SCOPE_PROF("native_call_native_group_norm.out");
            torch::executor::native::native_group_norm_out(context, input_base, weight_opt_out, bias_opt_out, N_base, C_base, HxW_base, group_base, eps_base, out0_base, out1_base, out2_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[8]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[9]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[10]);
    
            
        }
    ),
    
    Kernel(
        "aten::native_layer_norm.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& normalized_shape = *stack[1];
    	EValue& weight = *stack[2];
    	EValue& bias = *stack[3];
    	EValue& eps = *stack[4];
    	EValue& out0 = *stack[5];
    	EValue& out1 = *stack[6];
    	EValue& out2 = *stack[7];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> normalized_shape_list_out = normalized_shape.toIntList();
    	                
    	
    	    torch::executor::optional<torch::executor::Tensor> weight_opt_out = weight.toOptional<torch::executor::Tensor>();
    	            
    	
    	    torch::executor::optional<torch::executor::Tensor> bias_opt_out = bias.toOptional<torch::executor::Tensor>();
    	            
    	double eps_base = eps.to<double>();
    	torch::executor::Tensor & out0_base = out0.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out1_base = out1.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out2_base = out2.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_native_layer_norm.out");
            EXECUTORCH_SCOPE_PROF("native_call_native_layer_norm.out");
            torch::executor::native::native_layer_norm_out(context, input_base, normalized_shape_list_out, weight_opt_out, bias_opt_out, eps_base, out0_base, out1_base, out2_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[5]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    
            
        }
    ),
    
    Kernel(
        "aten::ne.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_ne.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_ne.Scalar_out");
            torch::executor::native::ne_scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::ne.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_ne.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_ne.Tensor_out");
            torch::executor::native::ne_tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::neg.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_neg.out");
            EXECUTORCH_SCOPE_PROF("native_call_neg.out");
            torch::executor::native::neg_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::nonzero.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_nonzero.out");
            EXECUTORCH_SCOPE_PROF("native_call_nonzero.out");
            torch::executor::native::nonzero_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::ones.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& size = *stack[0];
    	EValue& out = *stack[1];
    	
    	    torch::executor::ArrayRef<int64_t> size_list_out = size.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_ones.out");
            EXECUTORCH_SCOPE_PROF("native_call_ones.out");
            torch::executor::native::ones_out(context, size_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::permute_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dims = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> dims_list_out = dims.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_permute_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_permute_copy.out");
            torch::executor::native::permute_copy_out(context, self_base, dims_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::pixel_shuffle.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& upscale_factor = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t upscale_factor_base = upscale_factor.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_pixel_shuffle.out");
            EXECUTORCH_SCOPE_PROF("native_call_pixel_shuffle.out");
            torch::executor::native::pixel_shuffle_out(context, self_base, upscale_factor_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::pow.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& exponent = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Scalar & self_base = self.to<torch::executor::Scalar>();
    	const torch::executor::Tensor & exponent_base = exponent.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_pow.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_pow.Scalar_out");
            torch::executor::native::pow_Scalar_out(context, self_base, exponent_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::pow.Tensor_Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& exponent = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & exponent_base = exponent.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_pow.Tensor_Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_pow.Tensor_Scalar_out");
            torch::executor::native::pow_Tensor_Scalar_out(context, self_base, exponent_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::pow.Tensor_Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& exponent = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & exponent_base = exponent.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_pow.Tensor_Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_pow.Tensor_Tensor_out");
            torch::executor::native::pow_Tensor_Tensor_out(context, self_base, exponent_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::prod.int_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& dtype = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	bool keepdim_base = keepdim.to<bool>();
    	
    	    torch::executor::optional<torch::executor::ScalarType> dtype_opt_out = dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_prod.int_out");
            EXECUTORCH_SCOPE_PROF("native_call_prod.int_out");
            torch::executor::native::prod_int_out(context, self_base, dim_base, keepdim_base, dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::prod.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dtype = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::ScalarType> dtype_opt_out = dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_prod.out");
            EXECUTORCH_SCOPE_PROF("native_call_prod.out");
            torch::executor::native::prod_out(context, self_base, dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::reciprocal.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_reciprocal.out");
            EXECUTORCH_SCOPE_PROF("native_call_reciprocal.out");
            torch::executor::native::reciprocal_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::relu.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_relu.out");
            EXECUTORCH_SCOPE_PROF("native_call_relu.out");
            torch::executor::native::relu_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::remainder.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_remainder.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_remainder.Tensor_out");
            torch::executor::native::remainder_Tensor_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::remainder.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_remainder.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_remainder.Scalar_out");
            torch::executor::native::remainder_Scalar_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::repeat.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& repeats = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> repeats_list_out = repeats.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_repeat.out");
            EXECUTORCH_SCOPE_PROF("native_call_repeat.out");
            torch::executor::native::repeat_out(context, self_base, repeats_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::reflection_pad1d.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& padding = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> padding_list_out = padding.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_reflection_pad1d.out");
            EXECUTORCH_SCOPE_PROF("native_call_reflection_pad1d.out");
            torch::executor::native::reflection_pad1d_out(context, self_base, padding_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::reflection_pad2d.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& padding = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> padding_list_out = padding.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_reflection_pad2d.out");
            EXECUTORCH_SCOPE_PROF("native_call_reflection_pad2d.out");
            torch::executor::native::reflection_pad2d_out(context, self_base, padding_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::reflection_pad3d.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& padding = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> padding_list_out = padding.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_reflection_pad3d.out");
            EXECUTORCH_SCOPE_PROF("native_call_reflection_pad3d.out");
            torch::executor::native::reflection_pad3d_out(context, self_base, padding_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::replication_pad1d.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& padding = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> padding_list_out = padding.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_replication_pad1d.out");
            EXECUTORCH_SCOPE_PROF("native_call_replication_pad1d.out");
            torch::executor::native::replication_pad1d_out(context, self_base, padding_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::replication_pad2d.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& padding = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> padding_list_out = padding.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_replication_pad2d.out");
            EXECUTORCH_SCOPE_PROF("native_call_replication_pad2d.out");
            torch::executor::native::replication_pad2d_out(context, self_base, padding_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::replication_pad3d.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& padding = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> padding_list_out = padding.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_replication_pad3d.out");
            EXECUTORCH_SCOPE_PROF("native_call_replication_pad3d.out");
            torch::executor::native::replication_pad3d_out(context, self_base, padding_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::roll.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& shifts = *stack[1];
    	EValue& dims = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> shifts_list_out = shifts.toIntList();
    	                
    	
    	    torch::executor::ArrayRef<int64_t> dims_list_out = dims.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_roll.out");
            EXECUTORCH_SCOPE_PROF("native_call_roll.out");
            torch::executor::native::roll_out(context, self_base, shifts_list_out, dims_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::round.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_round.out");
            EXECUTORCH_SCOPE_PROF("native_call_round.out");
            torch::executor::native::round_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::rsqrt.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_rsqrt.out");
            EXECUTORCH_SCOPE_PROF("native_call_rsqrt.out");
            torch::executor::native::rsqrt_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::rsub.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& alpha = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & alpha_base = alpha.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_rsub.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_rsub.Scalar_out");
            torch::executor::native::rsub_scalar_out(context, self_base, other_base, alpha_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::scalar_tensor.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& s = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Scalar & s_base = s.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_scalar_tensor.out");
            EXECUTORCH_SCOPE_PROF("native_call_scalar_tensor.out");
            torch::executor::native::scalar_tensor_out(context, s_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::scatter_add.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& index = *stack[2];
    	EValue& src = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	const torch::executor::Tensor & index_base = index.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & src_base = src.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_scatter_add.out");
            EXECUTORCH_SCOPE_PROF("native_call_scatter_add.out");
            torch::executor::native::scatter_add_out(context, self_base, dim_base, index_base, src_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::select_copy.int_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& index = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	int64_t index_base = index.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_select_copy.int_out");
            EXECUTORCH_SCOPE_PROF("native_call_select_copy.int_out");
            torch::executor::native::select_copy_int_out(context, self_base, dim_base, index_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::select_scatter.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& src = *stack[1];
    	EValue& dim = *stack[2];
    	EValue& index = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & src_base = src.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	int64_t index_base = index.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_select_scatter.out");
            EXECUTORCH_SCOPE_PROF("native_call_select_scatter.out");
            torch::executor::native::select_scatter_out(context, self_base, src_base, dim_base, index_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::sigmoid.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_sigmoid.out");
            EXECUTORCH_SCOPE_PROF("native_call_sigmoid.out");
            torch::executor::native::sigmoid_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::sign.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_sign.out");
            EXECUTORCH_SCOPE_PROF("native_call_sign.out");
            torch::executor::native::sign_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::sin.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_sin.out");
            EXECUTORCH_SCOPE_PROF("native_call_sin.out");
            torch::executor::native::sin_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::sinh.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_sinh.out");
            EXECUTORCH_SCOPE_PROF("native_call_sinh.out");
            torch::executor::native::sinh_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::slice_copy.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& start = *stack[2];
    	EValue& end = *stack[3];
    	EValue& step = *stack[4];
    	EValue& out = *stack[5];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	
    	    torch::executor::optional<int64_t> start_opt_out = start.toOptional<int64_t>();
    	            
    	
    	    torch::executor::optional<int64_t> end_opt_out = end.toOptional<int64_t>();
    	            
    	int64_t step_base = step.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_slice_copy.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_slice_copy.Tensor_out");
            torch::executor::native::slice_copy_Tensor_out(context, self_base, dim_base, start_opt_out, end_opt_out, step_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[5]);
    
            
        }
    ),
    
    Kernel(
        "aten::slice_scatter.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& src = *stack[1];
    	EValue& dim = *stack[2];
    	EValue& start = *stack[3];
    	EValue& end = *stack[4];
    	EValue& step = *stack[5];
    	EValue& out = *stack[6];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & src_base = src.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	
    	    torch::executor::optional<int64_t> start_opt_out = start.toOptional<int64_t>();
    	            
    	
    	    torch::executor::optional<int64_t> end_opt_out = end.toOptional<int64_t>();
    	            
    	int64_t step_base = step.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_slice_scatter.out");
            EXECUTORCH_SCOPE_PROF("native_call_slice_scatter.out");
            torch::executor::native::slice_scatter_out(context, self_base, src_base, dim_base, start_opt_out, end_opt_out, step_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    
            
        }
    ),
    
    Kernel(
        "aten::split_copy.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& split_size = *stack[1];
    	EValue& dim = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t split_size_base = split_size.to<int64_t>();
    	int64_t dim_base = dim.to<int64_t>();
    	
    	    torch::executor::TensorList out_list_out = out.toTensorList();
    	                
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_split_copy.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_split_copy.Tensor_out");
            torch::executor::native::split_copy_Tensor_out(context, self_base, split_size_base, dim_base, out_list_out);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            stack[4] = &out;
        }
    ),
    
    Kernel(
        "aten::split_with_sizes_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& split_sizes = *stack[1];
    	EValue& dim = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> split_sizes_list_out = split_sizes.toIntList();
    	                
    	int64_t dim_base = dim.to<int64_t>();
    	
    	    torch::executor::TensorList out_list_out = out.toTensorList();
    	                
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_split_with_sizes_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_split_with_sizes_copy.out");
            torch::executor::native::split_with_sizes_copy_out(context, self_base, split_sizes_list_out, dim_base, out_list_out);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            stack[4] = &out;
        }
    ),
    
    Kernel(
        "aten::sqrt.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_sqrt.out");
            EXECUTORCH_SCOPE_PROF("native_call_sqrt.out");
            torch::executor::native::sqrt_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::squeeze_copy.dim_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_squeeze_copy.dim_out");
            EXECUTORCH_SCOPE_PROF("native_call_squeeze_copy.dim_out");
            torch::executor::native::squeeze_copy_dim_out(context, self_base, dim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::squeeze_copy.dims_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> dim_list_out = dim.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_squeeze_copy.dims_out");
            EXECUTORCH_SCOPE_PROF("native_call_squeeze_copy.dims_out");
            torch::executor::native::squeeze_copy_dims_out(context, self_base, dim_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::stack.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& tensors = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& out = *stack[2];
    	
    	    torch::executor::TensorList tensors_list_out = tensors.toTensorList();
    	                
    	int64_t dim_base = dim.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_stack.out");
            EXECUTORCH_SCOPE_PROF("native_call_stack.out");
            torch::executor::native::stack_out(context, tensors_list_out, dim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::sub.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& alpha = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & alpha_base = alpha.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_sub.out");
            EXECUTORCH_SCOPE_PROF("native_call_sub.out");
            torch::executor::native::sub_out(context, self_base, other_base, alpha_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::sub.Scalar_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& alpha = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_base = other.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & alpha_base = alpha.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_sub.Scalar_out");
            EXECUTORCH_SCOPE_PROF("native_call_sub.Scalar_out");
            torch::executor::native::sub_scalar_out(context, self_base, other_base, alpha_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::sum.IntList_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& keepdim = *stack[2];
    	EValue& dtype = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::ArrayRef<int64_t>> dim_opt_out = dim.toOptional<torch::executor::ArrayRef<int64_t>>();
    	            
    	bool keepdim_base = keepdim.to<bool>();
    	
    	    torch::executor::optional<torch::executor::ScalarType> dtype_opt_out = dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_sum.IntList_out");
            EXECUTORCH_SCOPE_PROF("native_call_sum.IntList_out");
            torch::executor::native::sum_dim_out(context, self_base, dim_opt_out, keepdim_base, dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::t_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_t_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_t_copy.out");
            torch::executor::native::t_copy_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::tan.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_tan.out");
            EXECUTORCH_SCOPE_PROF("native_call_tan.out");
            torch::executor::native::tan_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::tanh.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_tanh.out");
            EXECUTORCH_SCOPE_PROF("native_call_tanh.out");
            torch::executor::native::tanh_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::transpose_copy.int_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim0 = *stack[1];
    	EValue& dim1 = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim0_base = dim0.to<int64_t>();
    	int64_t dim1_base = dim1.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_transpose_copy.int_out");
            EXECUTORCH_SCOPE_PROF("native_call_transpose_copy.int_out");
            torch::executor::native::transpose_copy_int_out(context, self_base, dim0_base, dim1_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::tril.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& diagonal = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t diagonal_base = diagonal.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_tril.out");
            EXECUTORCH_SCOPE_PROF("native_call_tril.out");
            torch::executor::native::tril_out(context, self_base, diagonal_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::trunc.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_trunc.out");
            EXECUTORCH_SCOPE_PROF("native_call_trunc.out");
            torch::executor::native::trunc_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ),
    
    Kernel(
        "aten::unbind_copy.int_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	
    	    torch::executor::TensorList out_list_out = out.toTensorList();
    	                
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_unbind_copy.int_out");
            EXECUTORCH_SCOPE_PROF("native_call_unbind_copy.int_out");
            torch::executor::native::unbind_copy_int_out(context, self_base, dim_base, out_list_out);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            stack[3] = &out;
        }
    ),
    
    Kernel(
        "aten::unsqueeze_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	int64_t dim_base = dim.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_unsqueeze_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_unsqueeze_copy.out");
            torch::executor::native::unsqueeze_copy_out(context, self_base, dim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::var.correction_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& correction = *stack[2];
    	EValue& keepdim = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::ArrayRef<int64_t>> dim_opt_out = dim.toOptional<torch::executor::ArrayRef<int64_t>>();
    	            
    	
    	    torch::executor::optional<torch::executor::Scalar> correction_opt_out = correction.toOptional<torch::executor::Scalar>();
    	            
    	bool keepdim_base = keepdim.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_var.correction_out");
            EXECUTORCH_SCOPE_PROF("native_call_var.correction_out");
            torch::executor::native::var_correction_out(context, self_base, dim_opt_out, correction_opt_out, keepdim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::var.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& dim = *stack[1];
    	EValue& unbiased = *stack[2];
    	EValue& keepdim = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::optional<torch::executor::ArrayRef<int64_t>> dim_opt_out = dim.toOptional<torch::executor::ArrayRef<int64_t>>();
    	            
    	bool unbiased_base = unbiased.to<bool>();
    	bool keepdim_base = keepdim.to<bool>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_var.out");
            EXECUTORCH_SCOPE_PROF("native_call_var.out");
            torch::executor::native::var_out(context, self_base, dim_opt_out, unbiased_base, keepdim_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
        }
    ),
    
    Kernel(
        "aten::view_copy.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& self = *stack[0];
    	EValue& size = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	
    	    torch::executor::ArrayRef<int64_t> size_list_out = size.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_view_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_view_copy.out");
            torch::executor::native::view_copy_out(context, self_base, size_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    
            
        }
    ),
    
    Kernel(
        "aten::where.self_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& condition = *stack[0];
    	EValue& self = *stack[1];
    	EValue& other = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & condition_base = condition.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_where.self_out");
            EXECUTORCH_SCOPE_PROF("native_call_where.self_out");
            torch::executor::native::where_out(context, condition_base, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
        }
    ),
    
    Kernel(
        "aten::zeros.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& size = *stack[0];
    	EValue& out = *stack[1];
    	
    	    torch::executor::ArrayRef<int64_t> size_list_out = size.toIntList();
    	                
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
            internal::EventTracerProfileScope event_tracer_scope(context.internal_event_tracer(), "native_call_zeros.out");
            EXECUTORCH_SCOPE_PROF("native_call_zeros.out");
            torch::executor::native::zeros_out(context, size_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);
    
            
        }
    ), // Generated kernels
};

// Explicitly convert to ArrayRef, so that the API can take an empty C array of
// Kernels.
static KernelArrayRef kernel_array_ref(
    kernels_to_register,
    kernels_to_register + sizeof(kernels_to_register) / sizeof(Kernel));

// Return value not used. Keep the static variable assignment to register
// kernels in static initialization time.
static auto success_with_kernel_reg = register_kernels(kernel_array_ref);
} // namespace
} // namespace function
} // namespace executor
} // namespace torch
