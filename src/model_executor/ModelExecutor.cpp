/* Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 * Copyright 2023-2024 Arm Limited and/or its affiliates.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

#include <stdio.h>
#include <memory>
#include <vector>
#include <stdint.h>

#include <executorch/extension/data_loader/buffer_data_loader.h>
#include <executorch/extension/runner_util/inputs.h>
#include <executorch/runtime/executor/program.h>
#include <executorch/runtime/platform/log.h>
#include <executorch/runtime/platform/platform.h>
#include <executorch/runtime/platform/runtime.h>

/**
 * This header file is generated by the build process based on the .pte file
 * specified in the ET_PTE_FILE_PATH variable to the cmake build.
 * Control of the action of the .pte, it's use of operators and delegates, and
 * which are included in the bare metal build are also orchestrated by the
 * CMakeLists file. For example use see examples/arm/run.sh
 */
#include "fcn_temp_ozone/model_pte.h"
#include "utils/mbed_stats_wrapper.h"
#include "model_executor/ModelExecutor.h"

using namespace exec_aten;
using namespace std;
using torch::executor::Error;
using torch::executor::Result;

void et_pal_init(void) {}

__ET_NORETURN void et_pal_abort(void) {
  __builtin_trap();
}

/**
 * Emit a log message via platform output (serial port, console, etc).
 */
void et_pal_emit_log_message(
    __ET_UNUSED et_timestamp_t timestamp,
    et_pal_log_level_t level,
    const char* filename,
    __ET_UNUSED const char* function,
    size_t line,
    const char* message,
    __ET_UNUSED size_t length) {
  fprintf(stderr, "%c executorch:%s:%zu] %s\n", level, filename, line, message);
}

// Assume SystemCoreClock is defined correctly for your platform.
extern uint32_t SystemCoreClock;

void delay_ms(uint32_t ms) {
    uint32_t ticks = (SystemCoreClock / 1000) * ms;
    for (uint32_t i = 0; i < ticks; ++i) {
        __asm volatile("nop"); // nop is an assembly instruction that does nothing but consume one clock cycle.
    }
}

ModelExecutor::ModelExecutor(size_t pool_size)
    : m_method_allocator_pool(nullptr), m_allocator_pool_size(pool_size) {
}

ModelExecutor::~ModelExecutor() {
}

ModelExecutor& ModelExecutor::getInstance(size_t pool_size) {
    static ModelExecutor instance(pool_size); // Singleton instance with customizable pool size
    return instance;
}

std::vector<float> ModelExecutor::run_model(std::vector<float> feature_vector){

		torch::executor::runtime_init();

		ET_LOG(Info, "Model in %p %c", model_pte, model_pte[0]);
		
		/**
		 * Dynamically allocating a block of memory on the heap and assigning the address of the
		 * allocated memory to the member variable m_method_allocator_pool
		 */
		m_method_allocator_pool = (uint8_t*)malloc(m_allocator_pool_size);

		mbed_lib::print_memory_info("2");

		/**
		 * A DataLoader that wraps a pre-allocated buffer in network_model_sec. The FreeableBuffers
		 * that it returns do not actually free any data.
		 *
		 * This can be used to wrap data that is directly embedded into the firmware
		 * image, or to wrap data that was allocated elsewhere.
		 */
		// Create a loader to get the data of the program file. There are other
  		// DataLoaders that use mmap() or point to data that's already in memory, and
  		// users can create their own DataLoaders to load from arbitrary sources.
		// BufferDataLoader wraps a pre-allocated buffer. The FreeableBuffers
 		// that it returns do not actually free any data.
		auto loader = torch::executor::util::BufferDataLoader(model_pte, sizeof(model_pte));
		ET_LOG(Info, "Model PTE file loaded. Size: %lu bytes.", sizeof(model_pte));

		mbed_lib::print_memory_info("3");


		/**
		 * A deserialized ExecuTorch program binary.
		 * Loads a Program from the provided loader. The Program will hold a pointer
   		 * to the loader, which must outlive the returned Program instance.
		 */
		// Parse the program file. This is immutable, and can also be reused between
  		// multiple execution invocations across multiple threads.
		Result<torch::executor::Program> program =
			torch::executor::Program::load(&loader);
		if (!program.ok()) {
			ET_LOG(
				Info,
				"Program loading failed @ 0x%p: 0x%" PRIx32,
				model_pte,
				program.error());
		}

		ET_LOG(Info, "Model buffer loaded, has %lu methods", program->num_methods());

		mbed_lib::print_memory_info("4");

		const char* method_name = nullptr;
		{
			const auto method_name_result = program->get_method_name(0);
			ET_CHECK_MSG(method_name_result.ok(), "Program has no methods");
			method_name = *method_name_result;
		}
		ET_LOG(Info, "Running method %s", method_name);

		mbed_lib::print_memory_info("5");

		/**
		 * Gathers metadata for the named method.
		 */
		Result<torch::executor::MethodMeta> method_meta =
			program->method_meta(method_name);
		if (!method_meta.ok()) {
			ET_LOG(
				Info,
				"Failed to get method_meta for %s: 0x%x",
				method_name,
				(unsigned int)method_meta.error());
		}

		mbed_lib::print_memory_info("6");

		/**
		 * MemoryAllocator does simple allocation based on a size and returns the pointer
		 * to the memory address. It bookmarks a buffer with certain size. The
		 * allocation is simply checking space and growing the cur_ pointer with each
		 * allocation request.
		 *
		 * Simple example:
		 *
		 *   // User allocates a 100 byte long memory in the heap.
		 *   uint8_t* memory_pool = malloc(100 * sizeof(uint8_t));
		 *   MemoryAllocator allocator(100, memory_pool)
		 *   // Pass allocator object in the Executor
		 *
		 *   Underneath the hood, ExecuTorch will call
		 *   allocator.allocate() to keep iterating cur_ pointer
		 */
		// The runtime does not use malloc/new; it allocates all memory using the
		// MemoryManger provided by the client. Clients are responsible for allocating
		// the memory ahead of time, or providing MemoryAllocator subclasses that can
		// do it dynamically.
		// The method allocator is used to allocate all dynamic C++ metadata/objects
  		// used to represent the loaded method. This allocator is only used during
  	    // loading a method of the program, which will return an error if there was
        // not enough memory.
		// The amount of memory required depends on the loaded method and the runtime
        // code itself. The amount of memory here is usually determined by running the
        // method and seeing how much memory is actually used, though it's possible to
        // subclass MemoryAllocator so that it calls malloc() under the hood (see
        // MallocMemoryAllocator).
		// In this example we use a statically allocated memory pool.
		torch::executor::MemoryAllocator method_allocator{
			torch::executor::MemoryAllocator(
				m_allocator_pool_size, m_method_allocator_pool)};

		mbed_lib::print_memory_info("7");

		/**
		 * Stores unique pointers (std::unique_ptr) to dynamically allocated arrays of uint8_t.
		 * Owns the memory.
		 */
		// Each buffer typically corresponds to a different hardware memory bank. Most
		// mobile environments will only have a single buffer. Some embedded
		// environments may have more than one for, e.g., slow/large DRAM and
		// fast/small SRAM, or for memory associated with particular cores.
		std::vector<std::unique_ptr<uint8_t[]>> planned_buffers;

		mbed_lib::print_memory_info("8");

		/**
		 *
		 * Represent a reference to an array (0 or more elements
		 * consecutively in memory), i.e. a start pointer and a length.  It allows
		 * various APIs to take consecutive elements easily and conveniently.
		 *
		 * This class does not own the underlying data, it is expected to be used in
		 * situations where the data resides in some other buffer, whose lifetime
		 * extends past that of the Span.
		 *
		 * Span and ArrayRef are extrememly similar with the difference being ArrayRef
		 * views a list of constant elements and Span views a list of mutable elements.
		 * Clients should decide between the two based on if the list elements for their
		 * use case should be mutable.
		 *
		 * This is intended to be trivially copyable, so it should be passed by
		 * value.
		 *
		 * Passed to the allocator
		 */
		std::vector<torch::executor::Span<uint8_t>> planned_spans;

		mbed_lib::print_memory_info("9");

		/**
		 * Get the number of memory-planned buffers this method requires.
		 */
		size_t num_memory_planned_buffers = method_meta->num_memory_planned_buffers();

		for (size_t id = 0; id < num_memory_planned_buffers; ++id) {
			/**
			 * Get the size in bytes of the specified memory-planned buffer.
			 * Can only be changed in network definition (Python). Before exporting model.pte
			 */
			size_t buffer_size = static_cast<size_t>(method_meta->memory_planned_buffer_size(id).get());
			ET_LOG(Info, "Setting up planned buffer %zu, size %zu.", id, buffer_size);
			
			mbed_lib::print_memory_info("10");

			/**
			 * Dynamically allocates buffer_size bytes on the heap using std::make_unique<uint8_t[]>, 
			 * creating a std::unique_ptr to manage this memory. The std::unique_ptr is then 
			 * moved into the planned_buffers container, transferring ownership of the 
			 * heap-allocated memory to the container.
			 * No memory leaks since std::unique_ptr in planned_buffers owns the heap-allocated memory
			 * and deletes the memory when the unique_ptr is destroyed (e.g., when the vector is cleared, resized, or goes out of scope).
			 */
			planned_buffers.push_back(std::make_unique<uint8_t[]>(buffer_size));
			mbed_lib::print_memory_info("10.5");
			/**
			 * Simply provides a lightweight view over the existing heap-allocated 
			 * planned_buffers without taking ownership.
			 */
			planned_spans.push_back({planned_buffers.back().get(), buffer_size});
			mbed_lib::print_memory_info("11");

		}

		mbed_lib::print_memory_info("12");

		//mbed_lib::print_memory_info();
		torch::executor::HierarchicalAllocator planned_memory(
			{planned_spans.data(), planned_spans.size()});

		mbed_lib::print_memory_info("13");

		// Assemble all of the allocators into the MemoryManager that the Executor
  		// will use.
		torch::executor::MemoryManager memory_manager(
			&method_allocator, &planned_memory);

		mbed_lib::print_memory_info("14");

		// Load the method from the program, using the provided allocators. Running
		// the method can mutate the memory-planned buffers, so the method should only
		// be used by a single thread at at time, but it can be reused.
		Result<torch::executor::Method> method = program->load_method(method_name, &memory_manager);
		if (!method.ok()) {
			ET_LOG(
				Info,
				"Loading of method %s failed with status 0x%" PRIx32,
				method_name,
				method.error());
		}
		ET_LOG(Info, "Method loaded.");

		mbed_lib::print_memory_info("15");

		ET_LOG(Info, "Preparing inputs...");

		// Allocate input tensors and set all of their elements to 1. The `inputs`
  		// variable owns the allocated memory and must live past the last call to
  		// `execute()`.
		auto inputs = torch::executor::util::prepare_input_tensors(*method);
		if (!inputs.ok()) {
			ET_LOG(
				Info,
				"Preparing inputs tensors for method %s failed with status 0x%" PRIx32,
				method_name,
				inputs.error());
		}
		ET_LOG(Info, "Input prepared.");

		mbed_lib::print_memory_info("16");

		/*++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*/
		// // Set variable input
		torch::executor::EValue input_original = method->get_input(0);
    	Tensor tensor = input_original.payload.as_tensor;
    	float* data = input_original.payload.as_tensor.mutable_data_ptr<float>();

		ET_LOG(Info, "Number of input values required by model:%d", tensor.numel());

    	// // Change input
    	for(int j = 0; j < tensor.numel(); ++j){
        	data[j] = feature_vector[j];
    	}

		// // Set input 
		method->set_input(input_original,0);

		mbed_lib::print_memory_info("17");

		/*++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*/
		// Print Input

		size_t input_size = method->inputs_size();
    	const torch::executor::EValue input_new = method->get_input(0);
    	for (unsigned i = 0; i < input_size; ++i) {
        	Tensor te = input_new.payload.as_tensor;
        	for (int j = 0; j < te.numel(); ++j) { // numel returns the number of elements in the tensor
            	if (te.scalar_type() == ScalarType::Int) {
                	printf(
                    	"Input[%d][%d]: %d\n",
                    	i,
                    	j,
                    	te.const_data_ptr<int>()[j]);
            	} else {
                	printf(
                    	"Input[%d][%d]: %f\n",
                    	i,
                    	j,
                    	te.const_data_ptr<float>()[j]);
            	}
        	}
    	}

		/*++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*/
		
		ET_LOG(Info, "Starting the model execution...");
		//delay_ms(100);
	
		Error status = method->execute();
		if (status != Error::Ok) {
			ET_LOG(
				Info,
				"Execution of method %s failed with status 0x%" PRIx32,
				method_name,
				status);
		} else {
			ET_LOG(Info, "Model executed successfully.");
		}

		mbed_lib::print_memory_info("18");

		std::vector<torch::executor::EValue> outputs(method->outputs_size());
		ET_LOG(Info, "%zu outputs: ", outputs.size());
		status = method->get_outputs(outputs.data(), outputs.size());
		ET_CHECK(status == Error::Ok);
		std::vector<float> results; // Vector to hold the output values
		for (uint i = 0; i < outputs.size(); ++i) {
			Tensor t = outputs[i].toTensor();
			for (int j = 0; j < outputs[i].toTensor().numel(); ++j) {
			if (t.scalar_type() == ScalarType::Int) {
				printf(
					"Output[%d][%d]: %d\n",
					i,
					j,
					outputs[i].toTensor().const_data_ptr<int>()[j]);
				results.push_back(outputs[i].toTensor().const_data_ptr<int>()[j]);
			} else {
				printf(
					"Output[%d][%d]: %f\n",
					i,
					j,
					outputs[i].toTensor().const_data_ptr<float>()[j]);
				results.push_back(outputs[i].toTensor().const_data_ptr<float>()[j]);
			}
			}
		}
		ET_LOG(Info, "Program complete, exiting.");
		// Freeing the memory
		free(m_method_allocator_pool);
		m_method_allocator_pool = nullptr; // Optional but recommended to avoid dangling pointers
		//delay_ms(100); // Delays for 100 milliseconds

		mbed_lib::print_memory_info("19");

	return results;
}